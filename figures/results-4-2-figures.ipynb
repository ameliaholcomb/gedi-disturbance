{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylr2 import regress2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import spark_postgis\n",
    "from src import constants\n",
    "CORR_METHOD=\"pearson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_postgis.get_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occasionally, we get sets of three shots with a disturbance between.\n",
    "# Sometimes it is valid to count these as separate samples\n",
    "# (e.g. s1a -- disturbance -- s1b -- s2,\n",
    "# where the pair s1a-s2 is a treatment sample and s1b-s2 is a control sample).\n",
    "# But other times, it's really two measurements of the same sample\n",
    "# (e.g. s1a -- s1b -- disturbance -- s2, where s1a-s2 and s1b-s2 are both\n",
    "# measurements of the same disturbance event).\n",
    "# Just to be on the safe side, we can remove all the duplicates.\n",
    "# This function should be run on the control and treatment sets separately.\n",
    "\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    print(\n",
    "        \"Found {} s1 duplicates\".format(\n",
    "            len(df[df.duplicated(subset=[\"t1_shot_number\"])])\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Found {} s2 duplicates\".format(\n",
    "            len(df[df.duplicated(subset=[\"t2_shot_number\"])])\n",
    "        )\n",
    "    )\n",
    "    df = df.drop_duplicates(subset=[\"t1_shot_number\"], keep=\"first\")\n",
    "    df = df.drop_duplicates(subset=[\"t2_shot_number\"], keep=\"first\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Setup: Intact control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet((constants.RESULTS_PATH / \"gedi_neighbors_nau_test\").as_posix())\n",
    "sdf.createOrReplaceTempView(\"shots_table\")\n",
    "sdf = spark.sql(\"SELECT *, ST_GeomFromWKB(t1_geometry) AS t1_geom, ST_GeomFromWKB(t2_geometry) AS t2_geom FROM shots_table\")\n",
    "sdf = sdf.drop(\"t1_geometry\", \"t2_geometry\")\n",
    "print(sdf.count())\n",
    "sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def get_days(time_delta):\n",
    "  return time_delta.days\n",
    "\n",
    "sdf = sdf.withColumn(\"time_diff\", (sdf[\"t2_absolute_time\"] - sdf[\"t1_absolute_time\"]))\n",
    "sdf = sdf.withColumn(\"time_diff\", get_days(col(\"time_diff\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "sdf_filtered = sdf.filter(sdf.time_diff != 0)\n",
    "agbd_df = (\n",
    "    sdf_filtered.sample(withReplacement=False, fraction=0.1)\n",
    "    .select(\"t1_agbd_a0\", \"t2_agbd_a0\", \"time_diff\")\n",
    "    .toPandas()\n",
    ")\n",
    "rh98_df = (\n",
    "    sdf_filtered.sample(withReplacement=False, fraction=0.1)\n",
    "    .select(\"t1_rh_98_a0\", \"t2_rh_98_a0\", \"time_diff\")\n",
    "    .toPandas()\n",
    ")\n",
    "rh50_df = (\n",
    "    sdf_filtered.sample(withReplacement=False, fraction=0.1)\n",
    "    .select(\"t1_rh_50_a0\", \"t2_rh_50_a0\", \"time_diff\")\n",
    "    .toPandas()\n",
    ")\n",
    "n = sdf_filtered.count()\n",
    "print(n)\n",
    "corr_agbd = sdf_filtered.corr(\"t1_agbd_a0\", \"t2_agbd_a0\", method=CORR_METHOD)\n",
    "sdf = sdf.withColumn(\"agbd_diff\", (sdf.t2_agbd_a0 - sdf.t1_agbd_a0))\n",
    "bias_agbd = (\n",
    "    sdf_filtered.withColumn(\"agbd_diff\", (sdf.t2_agbd_a0 - sdf.t1_agbd_a0))\n",
    "    .select(mean(\"agbd_diff\"))\n",
    "    .collect()\n",
    ")[0][\"avg(agbd_diff)\"]\n",
    "reldiff_agbd = (\n",
    "    sdf_filtered.withColumn(\"agbd_reldiff\", (sdf.t2_agbd_a0 - sdf.t1_agbd_a0)/(sdf.t1_agbd_a0 + sdf.t2_agbd_a0))\n",
    "    .select(mean(\"agbd_reldiff\"))\n",
    "    .collect()\n",
    ")[0][\"avg(agbd_reldiff)\"]\n",
    "corr_rh98 = sdf_filtered.corr(\"t1_rh_98_a0\", \"t2_rh_98_a0\", method=CORR_METHOD)\n",
    "bias_rh98 = (\n",
    "    sdf_filtered.withColumn(\"rh98_diff\", (sdf.t2_rh_98_a0 - sdf.t1_rh_98_a0))\n",
    "    .select(mean(\"rh98_diff\"))\n",
    "    .collect()\n",
    ")[0][\"avg(rh98_diff)\"]\n",
    "reldiff_rh98 = (\n",
    "    sdf_filtered.withColumn(\"rh98_reldiff\", (sdf.t2_rh_98_a0 - sdf.t1_rh_98_a0)/(sdf.t1_rh_98_a0 + sdf.t2_rh_98_a0))\n",
    "    .select(mean(\"rh98_reldiff\"))\n",
    "    .collect()\n",
    ")[0][\"avg(rh98_reldiff)\"]\n",
    "corr_rh50 = sdf_filtered.corr(\"t1_rh_50_a0\", \"t2_rh_50_a0\", method=CORR_METHOD)\n",
    "bias_rh50 = (\n",
    "    sdf_filtered.withColumn(\"rh50_diff\", (sdf.t2_rh_50_a0 - sdf.t1_rh_50_a0))\n",
    "    .select(mean(\"rh50_diff\"))\n",
    "    .collect()\n",
    ")[0][\"avg(rh50_diff)\"]\n",
    "reldiff_rh50 = (\n",
    "    sdf_filtered.withColumn(\"rh50_reldiff\", (sdf.t2_rh_50_a0 - sdf.t1_rh_50_a0)/(sdf.t1_rh_50_a0 + sdf.t2_rh_50_a0))\n",
    "    .select(mean(\"rh50_reldiff\"))\n",
    "    .collect()\n",
    ")[0][\"avg(rh50_reldiff)\"]\n",
    "print(\"Control AGBD bias, corr, reldiff: {},{},{}\".format( bias_agbd, corr_agbd, reldiff_agbd))\n",
    "print(\"Control RH98 bias, corr, reldiff: {},{},{}\".format( bias_rh98, corr_rh98, reldiff_rh98))\n",
    "print(\"Control RH50 bias, corr, reldiff: {},{},{}\".format( bias_rh50, corr_rh50, reldiff_rh50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Setup: Disturbance (GLAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, col\n",
    "degrade_sdf = spark.read.parquet((constants.RESULTS_PATH / \"gedi_degradation_glad_0d\").as_posix())\n",
    "@udf(returnType=IntegerType())\n",
    "def get_days(time_delta):\n",
    "  return time_delta.days\n",
    "\n",
    "degrade_sdf = degrade_sdf.withColumn(\"time_diff\", (degrade_sdf[\"t2_absolute_time\"] - degrade_sdf[\"t1_absolute_time\"]))\n",
    "degrade_sdf = degrade_sdf.withColumn(\"time_diff\", get_days(col(\"time_diff\")))\n",
    "glad_df = gpd.GeoDataFrame(degrade_sdf.toPandas(), geometry=\"t2_geom\").copy()\n",
    "glad_df.loc[glad_df.control_disturbance > 0, \"sample_grp\"] = \"control\"\n",
    "# Note: points may have a control disturbance as well as a measured disturbance.\n",
    "# in that case, we include them in the treatment group; we don't care that they\n",
    "# were also disturbed at another, unmeasured time.\n",
    "glad_df.loc[glad_df.measured_disturbance > 0, \"sample_grp\"] = \"treatment\"\n",
    "print(len(glad_df))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"control\"]))\n",
    "control_df = remove_duplicates(glad_df[glad_df[\"sample_grp\"] == \"control\"])\n",
    "control_df[\"sample_grp\"] = \"control\"\n",
    "treatment_df = remove_duplicates(glad_df[glad_df[\"sample_grp\"] == \"treatment\"])\n",
    "treatment_df[\"sample_grp\"] = \"treatment\"\n",
    "glad_df = pd.concat([control_df, treatment_df])\n",
    "print(len(glad_df))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"control\"]))\n",
    "control_n = len(glad_df[glad_df[\"sample_grp\"] == \"control\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df = glad_df[glad_df[\"sample_grp\"] == \"control\"]\n",
    "glad_agbd_corr = control_df.t1_agbd_a0.corr(control_df.t2_agbd_a0, method=CORR_METHOD)\n",
    "glad_agbd_bias = (control_df.t2_agbd_a0 - control_df.t1_agbd_a0).mean()\n",
    "glad_agbd_reldiff = ((control_df.t2_agbd_a0 - control_df.t1_agbd_a0)/(control_df.t1_agbd_a0 + control_df.t2_agbd_a0)).mean()\n",
    "glad_rh98_corr = control_df.t1_rh_98_a0.corr(control_df.t2_rh_98_a0, method=CORR_METHOD)\n",
    "glad_rh98_bias = (control_df.t2_rh_98_a0 - control_df.t1_rh_98_a0).mean()\n",
    "glad_rh98_reldiff = ((control_df.t2_rh_98_a0 - control_df.t1_rh_98_a0)/(control_df.t1_rh_98_a0 + control_df.t2_rh_98_a0)).mean()\n",
    "glad_rh50_corr = control_df.t1_rh_50_a0.corr(control_df.t2_rh_50_a0, method=CORR_METHOD)\n",
    "glad_rh50_bias = (control_df.t2_rh_50_a0 - control_df.t1_rh_50_a0).mean()\n",
    "glad_rh50_reldiff = ((control_df.t2_rh_50_a0 - control_df.t1_rh_50_a0)/(control_df.t1_rh_50_a0 + control_df.t2_rh_50_a0))\n",
    "glad_rh50_reldiff.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "glad_rh50_reldiff = glad_rh50_reldiff.dropna().mean()\n",
    "print(\"Control AGBD bias, corr, reldiff: {},{},{}\".format(glad_agbd_bias, glad_agbd_corr, glad_agbd_reldiff))\n",
    "print(\"Control RH98 bias, corr, reldiff: {},{},{}\".format(glad_rh98_bias, glad_rh98_corr, glad_rh98_reldiff))\n",
    "print(\"Control RH50 bias, corr, reldiff: {},{},{}\".format(glad_rh50_bias, glad_rh50_corr, glad_rh50_reldiff))\n",
    "print(\"Mean t1 AGBD: {}\".format(control_df.t1_agbd_a0.mean()))\n",
    "print(\"Percent AGBD bias: {:.1f}%\".format(glad_agbd_bias / control_df.t1_agbd_a0.mean() * 100))\n",
    "del control_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Setup: Disturbance (AFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, col\n",
    "degrade_sdf = spark.read.parquet((constants.RESULTS_PATH / \"gedi_degradation_afc_2022\").as_posix())\n",
    "@udf(returnType=IntegerType())\n",
    "def get_days(time_delta):\n",
    "  return time_delta.days\n",
    "\n",
    "degrade_sdf = degrade_sdf.withColumn(\"time_diff\", (degrade_sdf[\"t2_absolute_time\"] - degrade_sdf[\"t1_absolute_time\"]))\n",
    "degrade_sdf = degrade_sdf.withColumn(\"time_diff\", get_days(col(\"time_diff\")))\n",
    "afc_df = gpd.GeoDataFrame(degrade_sdf.toPandas(), geometry=\"t2_geom\").copy()\n",
    "afc_df.loc[afc_df.control_disturbance > 0, \"sample_grp\"] = \"control\"\n",
    "# Note: points may have a control disturbance as well as a measured disturbance.\n",
    "# in that case, we include them in the treatment group; we don't care that they\n",
    "# were also disturbed at another, unmeasured time.\n",
    "afc_df.loc[afc_df.measured_disturbance > 0, \"sample_grp\"] = \"treatment\"\n",
    "print(len(afc_df))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"control\"]))\n",
    "control_df = remove_duplicates(afc_df[afc_df[\"sample_grp\"] == \"control\"])\n",
    "control_df[\"sample_grp\"] = \"control\"\n",
    "treatment_df = remove_duplicates(afc_df[afc_df[\"sample_grp\"] == \"treatment\"])\n",
    "treatment_df[\"sample_grp\"] = \"treatment\"\n",
    "afc_df = pd.concat([control_df, treatment_df])\n",
    "print(len(afc_df))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"control\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df = afc_df[afc_df[\"sample_grp\"] == \"control\"]\n",
    "afc_agbd_corr = control_df.t1_agbd_a0.corr(control_df.t2_agbd_a0, method=CORR_METHOD)\n",
    "afc_agbd_bias = (control_df.t2_agbd_a0 - control_df.t1_agbd_a0).mean()\n",
    "afc_agbd_reldiff = ((control_df.t2_agbd_a0 - control_df.t1_agbd_a0)/(control_df.t1_agbd_a0 + control_df.t2_agbd_a0)).mean()\n",
    "afc_rh98_corr = control_df.t1_rh_98_a0.corr(control_df.t2_rh_98_a0, method=CORR_METHOD)\n",
    "afc_rh98_bias = (control_df.t2_rh_98_a0 - control_df.t1_rh_98_a0).mean()\n",
    "afc_rh98_reldiff = ((control_df.t2_rh_98_a0 - control_df.t1_rh_98_a0)/(control_df.t1_rh_98_a0 + control_df.t2_rh_98_a0)).mean()\n",
    "afc_rh50_corr = control_df.t1_rh_50_a0.corr(control_df.t2_rh_50_a0, method=CORR_METHOD)\n",
    "afc_rh50_bias = (control_df.t2_rh_50_a0 - control_df.t1_rh_50_a0).mean()\n",
    "afc_rh50_reldiff = ((control_df.t2_rh_50_a0 - control_df.t1_rh_50_a0)/(control_df.t1_rh_50_a0 + control_df.t2_rh_50_a0))\n",
    "afc_rh50_reldiff.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "afc_rh50_reldiff = afc_rh50_reldiff.dropna().mean()\n",
    "print(\"Control AGBD bias, corr, reldiff: {},{},{}\".format(afc_agbd_bias, afc_agbd_corr, afc_agbd_reldiff))\n",
    "print(\"Control RH98 bias, corr, reldiff: {},{},{}\".format(afc_rh98_bias, afc_rh98_corr, afc_rh98_reldiff))\n",
    "print(\"Control RH50 bias, corr, reldiff: {},{},{}\".format(afc_rh50_bias, afc_rh50_corr, afc_rh50_reldiff))\n",
    "print(\"Mean t1 AGBD: {}\".format(control_df.t1_agbd_a0.mean()))\n",
    "print(\"Percent AGBD bias: {:.1f}%\".format(afc_agbd_bias / control_df.t1_agbd_a0.mean() * 100))\n",
    "del control_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "fig = plt.figure(layout='constrained', figsize=(20, 20))\n",
    "subfigs = fig.subfigures(2, 1, hspace=0.05, height_ratios=[0.85, 1])\n",
    "axs_top = subfigs[0].subplots(1, 2)\n",
    "axs_bottom = subfigs[1].subplots(1, 2)\n",
    "\n",
    "axi = axs_top[0]\n",
    "xs = rh98_df.t1_rh_98_a0\n",
    "ys = rh98_df.t2_rh_98_a0\n",
    "axmax = 50\n",
    "\n",
    "# increase gridsize for smaller hexagons\n",
    "imi = axi.hexbin(xs, ys, gridsize=100, cmap='magma', vmin=0, vmax=1200, mincnt=150)\n",
    "axi.axis([0, axmax, 0, axmax])\n",
    "axi.plot([0, axmax], [0, axmax], color='black', linestyle='dashed', linewidth=3)\n",
    "res = regress2(xs, ys, _method_type_2=\"reduced major axis\")\n",
    "dummy_xs = np.arange(axmax)\n",
    "axi.plot(dummy_xs, res[\"intercept\"] + res[\"slope\"] * dummy_xs, color='green', linewidth=3)\n",
    "\n",
    "# cb = fig.colorbar(imi, ax=axi, orientation='vertical')\n",
    "# cb.ax.tick_params(rotation=275)\n",
    "axi.set_xlabel(\"RH 98 1 (m)\", fontsize=22)\n",
    "axi.set_ylabel(\"RH 98 2 (m)\", fontsize=22)\n",
    "textstr = f\"Correlation: {corr_rh98:.2f}\\nBias: {bias_rh98:.2f}\\nSMA: y = {res['intercept']:.2f} + {res['slope']:.2f}x\"\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "axi.text(0.05, 0.95, textstr, transform=axi.transAxes, fontsize=22,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "axi = axs_top[1]\n",
    "xs = agbd_df.t1_agbd_a0\n",
    "ys = agbd_df.t2_agbd_a0\n",
    "axmax = 500\n",
    "\n",
    "# increase gridsize for smaller hexagons\n",
    "imi = axi.hexbin(xs, ys, gridsize=300, cmap='magma', vmin=0, vmax=1200, mincnt=150)\n",
    "axi.axis([0, axmax, 0, axmax])\n",
    "axi.plot([0, axmax], [0, axmax], color='black', linestyle='dashed', linewidth=3)\n",
    "res = regress2(xs, ys, _method_type_2=\"reduced major axis\")\n",
    "dummy_xs = np.arange(axmax)\n",
    "axi.plot(dummy_xs, res[\"intercept\"] + res[\"slope\"] * dummy_xs, color='green', linewidth=3)\n",
    "axi.set_xlabel(\"AGBD 1 (Mg/ha)\", fontsize=22)\n",
    "axi.set_ylabel(\"AGBD 2 (Mg/ha)\", fontsize=22)\n",
    "textstr = f\"Correlation: {corr_agbd:.2f}\\nBias: {bias_agbd:.2f}\\nSMA: y = {res['intercept']:.2f} + {res['slope']:.2f}x\"\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "axi.text(0.05, 0.95, textstr, transform=axi.transAxes, fontsize=22,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "subfigs[0].suptitle(f\"All near-coincident footprints ({n:,} shot pairs)\", fontsize=24)\n",
    "\n",
    "axi = axs_bottom[0]\n",
    "xs = glad_df[glad_df[\"sample_grp\"] == \"control\"].t1_rh_98_a0\n",
    "ys = glad_df[glad_df[\"sample_grp\"] == \"control\"].t2_rh_98_a0\n",
    "axmax = 50\n",
    "\n",
    "# increase gridsize for smaller hexagons\n",
    "imi = axi.hexbin(xs, ys, gridsize=100, cmap='magma', vmin=0, vmax=120, mincnt=6)\n",
    "axi.axis([0, axmax, 0, axmax])\n",
    "axi.plot([0, axmax], [0, axmax], color='black', linestyle='dashed', linewidth=3)\n",
    "res = regress2(xs, ys, _method_type_2=\"reduced major axis\")\n",
    "dummy_xs = np.arange(axmax)\n",
    "axi.plot(dummy_xs, res[\"intercept\"] + res[\"slope\"] * dummy_xs, color='green', linewidth=3)\n",
    "cb = fig.colorbar(imi, ax=axs_bottom.ravel().tolist(), orientation='horizontal', ticks=[0, 120], shrink=0.5)\n",
    "cb.ax.set_xticklabels(['Few pairs', 'Many pairs'])\n",
    "axi.set_xlabel(\"RH 98 1 (m)\", fontsize=22)\n",
    "axi.set_ylabel(\"RH 98 2 (m)\", fontsize=22)\n",
    "textstr = f\"Correlation: {glad_rh98_corr:.2f}\\nBias: {glad_rh98_bias:.2f}\\nSMA: y = {res['intercept']:.2f} + {res['slope']:.2f}x\"\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "axi.text(0.05, 0.95, textstr, transform=axi.transAxes, fontsize=22,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "axi = axs_bottom[1]\n",
    "xs = glad_df[glad_df[\"sample_grp\"] == \"control\"].t1_agbd_a0\n",
    "ys = glad_df[glad_df[\"sample_grp\"] == \"control\"].t2_agbd_a0\n",
    "axmax = 500\n",
    "\n",
    "# increase gridsize for smaller hexagons\n",
    "imi = axi.hexbin(xs, ys, gridsize=300, cmap='magma', vmin=0, vmax=120, mincnt=6)\n",
    "axi.axis([0, axmax, 0, axmax])\n",
    "axi.plot([0, axmax], [0, axmax], color='black', linestyle='dashed', linewidth=3)\n",
    "res = regress2(xs, ys, _method_type_2=\"reduced major axis\")\n",
    "dummy_xs = np.arange(axmax)\n",
    "axi.plot(dummy_xs, res[\"intercept\"] + res[\"slope\"] * dummy_xs, color='green', linewidth=3)\n",
    "axi.set_xlabel(\"AGBD 1 (Mg/ha)\", fontsize=22)\n",
    "axi.set_ylabel(\"AGBD 2 (Mg/ha)\", fontsize=22)\n",
    "textstr = f\"Correlation: {glad_agbd_corr:.2f}\\nBias: {glad_agbd_bias:.2f}\\nSMA: y = {res['intercept']:.2f} + {res['slope']:.2f}x\"\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "axi.text(0.05, 0.95, textstr, transform=axi.transAxes, fontsize=22,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "subfigs[1].suptitle(f\"Disturbed forest control ({control_n:,} shot pairs)\", fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity = \"rel_diff\"\n",
    "# Redo the above graph, but color by control group and have metric on the x-axis\n",
    "agbd_df[\"Control Group\"] = \"All pairs\"\n",
    "rh98_df[\"Control Group\"] = \"All pairs\"\n",
    "rh50_df[\"Control Group\"] = \"All pairs\"\n",
    "\n",
    "glad_df[\"Control Group\"] = \"Disturbed forest (GLAD)\"\n",
    "glad_df_agbd = glad_df[glad_df.sample_grp == \"control\"].copy()[[\"t1_agbd_a0\", \"t2_agbd_a0\", \"Control Group\"]]\n",
    "glad_df_rh98 = glad_df[glad_df.sample_grp == \"control\"].copy()[[\"t1_rh_98_a0\", \"t2_rh_98_a0\", \"Control Group\"]]\n",
    "glad_df_rh50 = glad_df[glad_df.sample_grp == \"control\"].copy()[[\"t1_rh_50_a0\", \"t2_rh_50_a0\", \"Control Group\"]]\n",
    "print(len(glad_df_agbd))\n",
    "print(len(glad_df_rh98))\n",
    "print(len(glad_df_rh50))\n",
    "\n",
    "afc_df[\"Control Group\"] = \"Disturbed forest (AFC)\"\n",
    "afc_df_agbd = afc_df[afc_df.sample_grp == \"control\"].copy()[[\"t1_agbd_a0\", \"t2_agbd_a0\", \"Control Group\"]]\n",
    "afc_df_rh98 = afc_df[afc_df.sample_grp == \"control\"].copy()[[\"t1_rh_98_a0\", \"t2_rh_98_a0\", \"Control Group\"]]\n",
    "afc_df_rh50 = afc_df[afc_df.sample_grp == \"control\"].copy()[[\"t1_rh_50_a0\", \"t2_rh_50_a0\", \"Control Group\"]]\n",
    "print(len(afc_df_agbd))\n",
    "print(len(afc_df_rh98))\n",
    "print(len(afc_df_rh50))\n",
    "\n",
    "big_agbd_df = pd.concat([agbd_df, glad_df_agbd, afc_df_agbd])\n",
    "big_rh98_df = pd.concat([rh98_df, glad_df_rh98, afc_df_rh98])\n",
    "big_rh50_df = pd.concat([rh50_df, glad_df_rh50, afc_df_rh50])\n",
    "\n",
    "big_agbd_df[\"rel_diff\"] = (big_agbd_df.t2_agbd_a0 - big_agbd_df.t1_agbd_a0) / (big_agbd_df.t1_agbd_a0 + big_agbd_df.t2_agbd_a0)\n",
    "big_rh98_df[\"rel_diff\"] = (big_rh98_df.t2_rh_98_a0 - big_rh98_df.t1_rh_98_a0) / (big_rh98_df.t1_rh_98_a0 + big_rh98_df.t2_rh_98_a0)\n",
    "big_rh50_df[\"rel_diff\"] = (big_rh50_df.t2_rh_50_a0 - big_rh50_df.t1_rh_50_a0) / (big_rh50_df.t1_rh_50_a0 + big_rh50_df.t2_rh_50_a0)\n",
    "\n",
    "big_agbd_df[\"pct_diff\"] = (big_agbd_df.t2_agbd_a0 - big_agbd_df.t1_agbd_a0) / (big_agbd_df.t1_agbd_a0 + 0.1) * 100\n",
    "big_rh98_df[\"pct_diff\"] = (big_rh98_df.t2_rh_98_a0 - big_rh98_df.t1_rh_98_a0) / (big_rh98_df.t1_rh_98_a0 + 0.1) * 100\n",
    "big_rh50_df[\"pct_diff\"] = (big_rh50_df.t2_rh_50_a0 - big_rh50_df.t1_rh_50_a0) / (big_rh50_df.t1_rh_50_a0 + 0.1) * 100\n",
    "\n",
    "big_agbd_df[\"Metric\"] = \"AGBD\"\n",
    "big_rh98_df[\"Metric\"] = \"RH 98\"\n",
    "big_rh50_df[\"Metric\"] = \"RH 50\"\n",
    "\n",
    "big_df = pd.concat([\n",
    "    big_rh98_df[[\"Metric\", \"Control Group\", quantity]],\n",
    "    big_rh50_df[[\"Metric\", \"Control Group\", quantity]],\n",
    "    big_agbd_df[[\"Metric\", \"Control Group\", quantity]],\n",
    "])\n",
    "\n",
    "print(big_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# dd=pd.melt(big_df, id_vars=['Metric'], value_vars=['All pairs', 'Disturbed forest\\n(RADD)', 'Disturbed forest\\n(AFC)'], var_name='Group')\n",
    "fig, axs = plt.subplots(1, 1, figsize=(12, 10))\n",
    "sns.boxplot(x='Metric',y=quantity, data=big_df, hue='Control Group', showfliers=False)\n",
    "\n",
    "iqrs = big_df.groupby(['Control Group','Metric']).describe()[quantity][['25%','75%']]\n",
    "print(iqrs)\n",
    "\n",
    "# Add labels to the IQR\n",
    "label_offset = 0.275     # unfortunately this needs to be adjusted manually\n",
    "for iqr in iqrs.iterrows():\n",
    "    name, metric = iqr[0]\n",
    "    if 'RH 98' in metric:\n",
    "        loc = 0\n",
    "    if 'RH 50' in metric:\n",
    "        loc = 1\n",
    "    if 'AGBD' in metric:\n",
    "        loc = 2\n",
    "    if 'All' in name:\n",
    "        loc -= label_offset\n",
    "    if 'GLAD' in name:\n",
    "        loc += 0\n",
    "    if 'AFC' in name:\n",
    "        loc += label_offset\n",
    "    axs.text(loc, iqr[1][0], str(round(iqr[1][0],2)), color='white', \n",
    "             bbox=dict(facecolor='black', alpha=0.5, edgecolor='black'), ha='center')\n",
    "    axs.text(loc, iqr[1][1], str(round(iqr[1][1],2)), color='white', \n",
    "             bbox=dict(facecolor='black', alpha=0.5, edgecolor='black'), ha='center')\n",
    "\n",
    "axs.plot([-0.5, 2.5], [0, 0], color='black', linestyle='dashed', alpha = 0.5, linewidth = 2)\n",
    "axs.set_xticklabels(axs.get_xticklabels(), fontsize=14)\n",
    "if quantity == \"rel_diff\":\n",
    "    axs.set_ylabel(\"Relative difference\", fontsize=18)\n",
    "elif quantity == \"pct_diff\":\n",
    "    axs.set_ylabel(\"Percent difference\", fontsize=18)\n",
    "axs.set_xlabel(\"Metric\", fontsize=18)\n",
    "plt.setp(axs.get_legend().get_texts(), fontsize='16') # for legend text\n",
    "plt.setp(axs.get_legend().get_title(), fontsize='16') # for legend title"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
