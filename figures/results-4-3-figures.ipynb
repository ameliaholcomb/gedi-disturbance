{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylr2 import regress2\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, col\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import rioxarray as rxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import constants\n",
    "from src.data import spark_postgis\n",
    "from src.utils import raster_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_postgis.get_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occasionally, we get sets of three shots with a disturbance between.\n",
    "# Sometimes it is valid to count these as separate samples\n",
    "# (e.g. s1a -- disturbance -- s1b -- s2,\n",
    "# where the pair s1a-s2 is a treatment sample and s1b-s2 is a control sample).\n",
    "# But other times, it's really two measurements of the same sample\n",
    "# (e.g. s1a -- s1b -- disturbance -- s2, where s1a-s2 and s1b-s2 are both\n",
    "# measurements of the same disturbance event).\n",
    "# Just to be on the safe side, we can remove all the duplicates.\n",
    "# This function should be run on the control and treatment sets separately.\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    print(\n",
    "        \"Found {} s1 duplicates\".format(\n",
    "            len(df[df.duplicated(subset=[\"t1_shot_number\"])])\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Found {} s2 duplicates\".format(\n",
    "            len(df[df.duplicated(subset=[\"t2_shot_number\"])])\n",
    "        )\n",
    "    )\n",
    "    df = df.drop_duplicates(subset=[\"t1_shot_number\"], keep=\"first\")\n",
    "    df = df.drop_duplicates(subset=[\"t2_shot_number\"], keep=\"first\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrade_sdf = spark.read.parquet(\n",
    "    (constants.RESULTS_PATH / \"gedi_degradation_glad_0d_l24a\").as_posix()\n",
    ")\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def get_days(time_delta):\n",
    "    return time_delta.days\n",
    "\n",
    "\n",
    "degrade_sdf = degrade_sdf.withColumn(\n",
    "    \"time_diff\",\n",
    "    (degrade_sdf[\"t2_absolute_time\"] - degrade_sdf[\"t1_absolute_time\"]),\n",
    ")\n",
    "degrade_sdf = degrade_sdf.withColumn(\"time_diff\", get_days(col(\"time_diff\")))\n",
    "glad_df = gpd.GeoDataFrame(degrade_sdf.toPandas(), geometry=\"t2_geom\").copy()\n",
    "glad_df.loc[glad_df.control_disturbance > 0, \"sample_grp\"] = \"control\"\n",
    "# Note: points may have a control disturbance as well as a measured disturbance.\n",
    "# in that case, we include them in the treatment group; we don't care that they\n",
    "# were also disturbed at another, unmeasured time.\n",
    "glad_df.loc[glad_df.measured_disturbance > 0, \"sample_grp\"] = \"treatment\"\n",
    "print(len(glad_df))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"control\"]))\n",
    "control_df = remove_duplicates(glad_df[glad_df[\"sample_grp\"] == \"control\"])\n",
    "control_df[\"sample_grp\"] = \"control\"\n",
    "treatment_df = remove_duplicates(glad_df[glad_df[\"sample_grp\"] == \"treatment\"])\n",
    "treatment_df[\"sample_grp\"] = \"treatment\"\n",
    "glad_df = pd.concat([control_df, treatment_df])\n",
    "print(len(glad_df))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(glad_df[glad_df[\"sample_grp\"] == \"control\"]))\n",
    "control_n = len(glad_df[glad_df[\"sample_grp\"] == \"control\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(glad_df[glad_df.sample_grp == \"treatment\"]).drop(columns=[\"t1_geom\"]).to_feather(\n",
    "    constants.RESULTS_PATH / \"glad_treatment_0d.feather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrade_sdf = spark.read.parquet(\n",
    "    (constants.RESULTS_PATH / \"gedi_degradation_afc_2022\").as_posix()\n",
    ")\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def get_days(time_delta):\n",
    "    return time_delta.days\n",
    "\n",
    "degrade_sdf = degrade_sdf.withColumn(\n",
    "    \"time_diff\",\n",
    "    (degrade_sdf[\"t2_absolute_time\"] - degrade_sdf[\"t1_absolute_time\"]),\n",
    ")\n",
    "degrade_sdf = degrade_sdf.withColumn(\"time_diff\", get_days(col(\"time_diff\")))\n",
    "afc_df = gpd.GeoDataFrame(degrade_sdf.toPandas(), geometry=\"t2_geom\").copy()\n",
    "afc_df.loc[afc_df.control_disturbance > 0, \"sample_grp\"] = \"control\"\n",
    "# Note: points may have a control disturbance as well as a measured disturbance.\n",
    "# in that case, we include them in the treatment group; we don't care that they\n",
    "# were also disturbed at another, unmeasured time.\n",
    "afc_df.loc[afc_df.measured_disturbance > 0, \"sample_grp\"] = \"treatment\"\n",
    "print(len(afc_df))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"control\"]))\n",
    "control_df = remove_duplicates(afc_df[afc_df[\"sample_grp\"] == \"control\"])\n",
    "control_df[\"sample_grp\"] = \"control\"\n",
    "treatment_df = remove_duplicates(afc_df[afc_df[\"sample_grp\"] == \"treatment\"])\n",
    "treatment_df[\"sample_grp\"] = \"treatment\"\n",
    "afc_df = pd.concat([control_df, treatment_df])\n",
    "print(len(afc_df))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(afc_df[afc_df[\"sample_grp\"] == \"control\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_adjust(df_orig):\n",
    "    bins = np.arange(0, 1200, 60)\n",
    "    df_orig[\"time_diff_bin\"] = pd.cut(df_orig[\"time_diff\"], bins=bins)\n",
    "    treatment_dist = df_orig[df_orig.sample_grp == \"treatment\"].groupby(\"time_diff_bin\").size().reset_index(name=\"count\")\n",
    "\n",
    "    new_control = []\n",
    "    for i in range(0, len(treatment_dist)):\n",
    "        bin, count = treatment_dist.iloc[i][\"time_diff_bin\"], treatment_dist.iloc[i][\"count\"]\n",
    "        d = df_orig[(df_orig.sample_grp == \"control\") & (df_orig.time_diff_bin == bin)]\n",
    "        if len(d) < count:\n",
    "            count = len(d)\n",
    "        new_control.append(d.sample(count, replace=False))\n",
    "\n",
    "    new_control = pd.concat(new_control)\n",
    "    return pd.concat([new_control, df_orig[df_orig.sample_grp == \"treatment\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing.\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "degrade_df = temporal_adjust(afc_df)\n",
    "\n",
    "dist_control = degrade_df[degrade_df.sample_grp == \"control\"].t2_agbd_a0 - degrade_df[degrade_df.sample_grp == \"control\"].t1_agbd_a0\n",
    "dist_treatment = degrade_df[degrade_df.sample_grp == \"treatment\"].t2_agbd_a0 - degrade_df[degrade_df.sample_grp == \"treatment\"].t1_agbd_a0\n",
    "\n",
    "# Plot distribution of S2 - S1 for control and treatment groups.\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
    "axi = ax[0]\n",
    "axi.hist(dist_control, bins=50)\n",
    "\n",
    "axi = ax[1]\n",
    "axi.hist(dist_treatment, bins=50)\n",
    "\n",
    "# Welch's t-test. Can we reject the null hypotheses that \n",
    "# (1) the mean of the treatment group is equal to the mean of the control group?\n",
    "res = stats.ttest_ind(dist_control, dist_treatment, equal_var=False)\n",
    "print(res.pvalue)\n",
    "# (2) the mean of the control group is greater than the mean of the treatment group?\n",
    "res = stats.ttest_ind(dist_control, dist_treatment, equal_var=False, alternative=\"greater\")\n",
    "print(res.pvalue)\n",
    "\n",
    "# Does it also hold if we look only at single-pixel disturbances?\n",
    "dist_treatment = degrade_df[(degrade_df.sample_grp == \"treatment\") & (degrade_df.measured_disturbance == 1)].t2_agbd_a0 - degrade_df[(degrade_df.sample_grp == \"treatment\") & (degrade_df.measured_disturbance == 1)].t1_agbd_a0\n",
    "\n",
    "# Welch's t-test\n",
    "# (1) the mean of the treatment group is equal to the mean of the control group?\n",
    "res = stats.ttest_ind(dist_control, dist_treatment, equal_var=False)\n",
    "print(res.pvalue)\n",
    "# (2) the mean of the treatment group is greater than the mean of the control group?\n",
    "res = stats.ttest_ind(dist_control, dist_treatment, equal_var=False, alternative=\"greater\")\n",
    "print(res.pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Biomass loss depends on ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afc_df = afc_df[afc_df.t1_agbd_a0 != 0]\n",
    "glad_df = glad_df[glad_df.t1_agbd_a0 != 0]\n",
    "afc_df[\"agbd_pct_loss\"] = (afc_df.t1_agbd_a0 - afc_df.t2_agbd_a0) / (afc_df.t1_agbd_a0) * 100\n",
    "glad_df[\"agbd_pct_loss\"] = (glad_df.t1_agbd_a0 - glad_df.t2_agbd_a0) / (glad_df.t1_agbd_a0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# 0. Days since disturbance for disturbed samples\n",
    "treatment_df = glad_df[glad_df.sample_grp == \"treatment\"]\n",
    "disturb_days = np.array([treatment_df.p1_disturb_date, treatment_df.p2_disturb_date, treatment_df.p3_disturb_date, treatment_df.p4_disturb_date, treatment_df.p5_disturb_date, treatment_df.p6_disturb_date, treatment_df.p7_disturb_date, treatment_df.p8_disturb_date, treatment_df.p9_disturb_date]).T.astype(float)\n",
    "disturb_days[disturb_days == 0] = np.nan\n",
    "mode_disturb_days = stats.mode(disturb_days, axis=1, nan_policy='omit', keepdims=True)[0].squeeze()\n",
    "treatment_df[\"mode_disturb_days\"] = mode_disturb_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_df[\"days_since_disturbance\"] = treatment_df.t2_days - treatment_df.mode_disturb_days\n",
    "# At the moment, because of the way we record the disturbance date, some of the days_since_disturbance\n",
    "# values are negative, even in the treatment set.\n",
    "# This is because the control disturb_date is also recorded whenever it is applicable\n",
    "# (and AFTER the treatment disturb_date in the code, so it will override the treatment disturb_date).\n",
    "# Many treatment disturbance pairs also have unmeasured disturbances at another time.\n",
    "# For now, only include disturbances with a positive number of days since disturbance.\n",
    "days_since_disturb_df = treatment_df[treatment_df.days_since_disturbance >= 0]\n",
    "days_since_disturb_df[\"months_since_disturbance\"] = days_since_disturb_df.days_since_disturbance // 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrade_sdf = spark.read.parquet(\n",
    "    (constants.RESULTS_PATH / \"gedi_degradation_afc_intensity\").as_posix()\n",
    ")\n",
    "\n",
    "intensity_df = gpd.GeoDataFrame(\n",
    "    degrade_sdf.toPandas(), geometry=\"t2_geom\"\n",
    ").copy()\n",
    "intensity_df = intensity_df[intensity_df[\"t1_agbd_a0\"] != 0]\n",
    "intensity_df.loc[intensity_df.control_disturbance > 0, \"sample_grp\"] = \"control\"\n",
    "# Note: points may have a control disturbance as well as a measured disturbance.\n",
    "# in that case, we include them in the treatment group; we don't care that they\n",
    "# were also disturbed at another, unmeasured time.\n",
    "intensity_df.loc[\n",
    "    intensity_df.measured_disturbance > 0, \"sample_grp\"\n",
    "] = \"treatment\"\n",
    "print(len(intensity_df))\n",
    "print(len(intensity_df[intensity_df[\"sample_grp\"] == \"treatment\"]))\n",
    "print(len(intensity_df[intensity_df[\"sample_grp\"] == \"control\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_df = intensity_df[(intensity_df.sample_grp == \"treatment\")]\n",
    "# intensity_df = remove_duplicates(intensity_df)\n",
    "# print(len(intensity_df))\n",
    "intensity_df[\"agbd_pct_loss\"] = (intensity_df.t1_agbd_a0 - intensity_df.t2_agbd_a0) / (intensity_df.t1_agbd_a0) * 100\n",
    "intensity_df.disturb_intensity_old = intensity_df.disturb_intensity.copy()\n",
    "intensity_df.disturb_intensity = intensity_df.disturb_intensity * 4 / intensity_df.measured_disturbance\n",
    "\n",
    "bins = np.concatenate([np.arange(0, 0.1, 0.025), np.arange(0.1,0.71, 0.1)])\n",
    "print(bins)\n",
    "intensity_df[\"disturb_bin\"] = pd.cut(intensity_df.disturb_intensity, bins=bins, labels=bins[:-1])\n",
    "print(intensity_df[['disturb_intensity', 'disturb_bin']])\n",
    "print(intensity_df.disturb_bin.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now bootstrap and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_and_plot_extent(degrade_df, axi):\n",
    "    treatment_df = degrade_df[degrade_df.sample_grp == \"treatment\"]\n",
    "    control_df = degrade_df[degrade_df.sample_grp == \"control\"]\n",
    "    # Bootstrap confidence intervals for the median percent AGBD change.\n",
    "    k_dist = np.max(degrade_df.measured_disturbance)\n",
    "    # 1. Bootstrap sample the median pct delta AGBD for each disturbance extent and the control.\n",
    "    n = 100\n",
    "    medians = np.zeros((n, k_dist+1))\n",
    "    for i in range(n):\n",
    "        medians[i, 0] = np.median(control_df.agbd_pct_loss.sample(frac=1, replace=True))\n",
    "        for j in range(k_dist):\n",
    "            medians[i, j+1] = np.median(treatment_df[treatment_df.measured_disturbance == j+1].agbd_pct_loss.sample(frac=1, replace=True))\n",
    "    print(medians.mean(axis=0))\n",
    "\n",
    "    # 2. Compute the 95% confidence interval for each median for plotting.\n",
    "    ci = np.zeros((2, k_dist+1))\n",
    "    for i in range(k_dist+1):\n",
    "        ci[:, i] = np.percentile(medians[:, i], [2.5, 97.5])\n",
    "    print(ci)\n",
    "\n",
    "    # 3. Compute a regression line for disturbance extent vs. median pct delta AGBD.\n",
    "    intercepts = np.zeros(n)\n",
    "    slopes = np.zeros(n)\n",
    "    rs = np.zeros(n)\n",
    "    pvals = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        model = sm.OLS(medians[i, 1:k_dist+1], sm.add_constant(np.arange(1, k_dist+1)))\n",
    "        results = model.fit()\n",
    "        intercepts[i] = results.params[0]\n",
    "        slopes[i] = results.params[1]\n",
    "        rs[i] = results.rsquared\n",
    "        pvals[i] = results.pvalues[1]\n",
    "\n",
    "    # 4. Plot the distribution of median pct delta AGBD for each disturbance extent.\n",
    "    axi.errorbar(np.arange(0, k_dist+1), medians.mean(axis=0), yerr=np.abs(ci - medians.mean(axis=0)), fmt='o', ecolor=\"tab:blue\", capsize=3, zorder=1)\n",
    "    axi.scatter(0, medians.mean(axis=0)[0], color=\"tab:green\", zorder=10)\n",
    "    axi.scatter(np.arange(1, k_dist+1), medians.mean(axis=0)[1:k_dist+1], color=\"tab:orange\", zorder=10)\n",
    "    # Plot a sample of the regression lines\n",
    "    sample = np.random.choice(np.arange(n), size=25)\n",
    "    for i in sample:\n",
    "        axi.plot(np.arange(1, k_dist+1), intercepts[i] + slopes[i] * np.arange(1, k_dist+1), color=\"gray\", alpha=0.2, zorder=5)\n",
    "    axi.plot(np.arange(1, k_dist+1), intercepts.mean() + slopes.mean() * np.arange(1, k_dist+1), color=\"black\", zorder=6)\n",
    "\n",
    "    axi.set_xticks(np.arange(0, k_dist+1))\n",
    "    axi.set_xlabel(f\"Disturbance extent (1-{k_dist})\")\n",
    "    axi.set_ylabel(\"Median percent AGBD loss\")\n",
    "    axi.set_title(f\"Biomass loss by disturbance extent (1-{k_dist})\")\n",
    "\n",
    "    legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='tab:orange', label=\"Treatment\",\n",
    "                markerfacecolor='tab:orange', markersize=7, linestyle='None'),\n",
    "            Line2D([0], [0], marker='o', color='tab:green', label=\"Control\",\n",
    "                markerfacecolor='tab:green', markersize=7, linestyle='None'),\n",
    "            Line2D([0], [0], color='black', linewidth=3, label=f\"y = {slopes.mean():.1f}x + {intercepts.mean():.1f}\\n$R^2$ = {rs.mean():.2f}, p = {pvals.mean():.3f}\"),\n",
    "        ]\n",
    "    axi.legend(loc=\"lower right\", handles=legend_elements)\n",
    "    print(rs.mean())\n",
    "    print(pvals.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_and_plot_time(days_since_disturb_df, axi):\n",
    "    k_months = 23 # 0-22 months since disturbance. Beyond 22 months we don't have enough data.\n",
    "    # 1. Bootstrap sample the median pct delta AGBD for each number of months since disturbance.\n",
    "    n = 100\n",
    "\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        # print(days_since_disturb_df.months_since_disturbance.value_counts().sort_index())\n",
    "    medians = np.zeros((n, k_months))\n",
    "    for i in range(n):\n",
    "        for j in range(k_months):\n",
    "            medians[i, j] = np.median(days_since_disturb_df[days_since_disturb_df.months_since_disturbance == j].agbd_pct_loss.sample(frac=1, replace=True))\n",
    "\n",
    "    # 2. Compute the 95% confidence interval for each median for plotting.\n",
    "    ci = np.zeros((2, k_months))\n",
    "    for i in range(k_months):\n",
    "        ci[:, i] = np.percentile(medians[:, i], [2.5, 97.5])\n",
    "\n",
    "    # 3. Plot the distribution of median pct delta AGBD for each time since disturbance.\n",
    "\n",
    "    axi.errorbar(np.arange(k_months), medians.mean(axis=0), yerr=np.abs(ci - medians.mean(axis=0)), fmt='o', ecolor=\"tab:blue\", capsize=3, zorder=1)\n",
    "    axi.scatter(0, medians.mean(axis=0)[0], color=\"tab:green\", zorder=10)\n",
    "    axi.scatter(np.arange(k_months), medians.mean(axis=0), color=\"tab:orange\", zorder=10)\n",
    "\n",
    "    # Try fitting a curve ...\n",
    "    # intercept, linear, quadratic\n",
    "    coeffs = np.zeros((n, 3))\n",
    "    adjrs = np.zeros(n)\n",
    "    # intercept, linear, quadratic\n",
    "    ps = np.zeros((n, 3))\n",
    "    for i in range(n):\n",
    "        data = pd.DataFrame({'y': medians[i, :], 'x': np.arange(k_months)})\n",
    "        model = smf.ols(formula = 'y ~ np.power(x, 2) + x', data=data)\n",
    "        # model = smf.ols(formula = 'y ~ x', data=data)\n",
    "        results = model.fit()\n",
    "        coeffs[i, :] = results.params\n",
    "        adjrs[i] = results.rsquared_adj\n",
    "        ps[i] = results.pvalues[:3]\n",
    "\n",
    "    print(coeffs.mean(axis=0))\n",
    "    sample = np.random.choice(np.arange(n), size=25)\n",
    "    for i in sample:\n",
    "        axi.plot(np.arange(k_months), coeffs[i, 0] + coeffs[i, 1] * np.power(np.arange(k_months), 2) + coeffs[i, 2] * np.arange(k_months), color=\"gray\", alpha=0.2, zorder=5)\n",
    "    axi.plot(np.arange(k_months), coeffs.mean(axis=0)[0] + coeffs.mean(axis=0)[1] * np.power(np.arange(k_months), 2) + coeffs.mean(axis=0)[2] * np.arange(k_months), color=\"black\", linewidth=3, zorder=6)\n",
    "\n",
    "    axi.set_xticks(np.arange(0, k_months, 3))\n",
    "    axi.set_xlabel(f\"Months (rounded) since disturbance\")\n",
    "    axi.set_ylabel(\"Median percent AGBD loss\")\n",
    "    axi.set_ylim(0, None)\n",
    "    axi.set_title(f\"Biomass loss by time since disturbance\")\n",
    "\n",
    "    legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='tab:orange', label=\"Treatment\",\n",
    "                markerfacecolor='tab:orange', markersize=7, linestyle='None'),\n",
    "            Line2D([0], [0], color='black', linewidth=3, label=f\"y = {coeffs.mean(axis=0)[0]:.1f} + {coeffs.mean(axis=0)[1]:.1f}x + {coeffs.mean(axis=0)[2]:.1f}$x^2$\\nadj-$R^2$ = {adjrs.mean():.2f}\"),\n",
    "        ]\n",
    "    axi.legend(loc=\"lower right\", handles=legend_elements)\n",
    "    print(adjrs.mean())\n",
    "    print(ps.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_and_plot_intensity(degrade_df, bins, axi):\n",
    "    import statsmodels.formula.api as smf\n",
    "\n",
    "    # Bootstrap confidence intervals for the median percent AGBD change.\n",
    "    k_dist = len(bins) - 1\n",
    "    bin_vals = bins[:-1]\n",
    "\n",
    "    # 1. Bootstrap sample the median pct delta AGBD for each disturbance extent and the control.\n",
    "    n = 100\n",
    "    medians = np.zeros((n, k_dist))\n",
    "    for i in range(n):\n",
    "        for j, b in enumerate(bin_vals):\n",
    "            medians[i, j] = np.median(degrade_df[degrade_df.disturb_bin == b].agbd_pct_loss.sample(frac=1, replace=True))\n",
    "    print(medians.mean(axis=0))\n",
    "\n",
    "    # 2. Compute the 95% confidence interval for each median for plotting.\n",
    "    ci = np.zeros((2, k_dist))\n",
    "    for i in range(k_dist):\n",
    "        ci[:, i] = np.percentile(medians[:, i], [2.5, 97.5])\n",
    "    print(ci)\n",
    "\n",
    "    # 3. Compute a regression line for disturbance extent vs. median pct delta AGBD.\n",
    "    intercepts = np.zeros(n)\n",
    "    slopes = np.zeros(n)\n",
    "    rs = np.zeros(n)\n",
    "    pvals = np.zeros(n)\n",
    "\n",
    "    def fun(a, b, xs):\n",
    "        return a + b * np.sqrt(xs)\n",
    "\n",
    "    for i in range(n):\n",
    "        df = pd.DataFrame({'intensity': bin_vals, 'agbd_pct_loss': medians[i,:]})\n",
    "        model = smf.ols('agbd_pct_loss ~ np.sqrt(intensity)', df)\n",
    "        results = model.fit()\n",
    "        intercepts[i] = results.params[0]\n",
    "        slopes[i] = results.params[1]\n",
    "        rs[i] = results.rsquared\n",
    "        pvals[i] = results.pvalues[1]\n",
    "\n",
    "    # 4. Plot the distribution of median pct delta AGBD for each disturbance extent.\n",
    "    axi.errorbar(bin_vals, medians.mean(axis=0), yerr=np.abs(ci - medians.mean(axis=0)), fmt='o', ecolor=\"tab:blue\", capsize=3, zorder=1)\n",
    "    axi.scatter(bin_vals, medians.mean(axis=0), color=\"tab:orange\", zorder=10)\n",
    "    # Plot a sample of the regression lines\n",
    "    sample = np.random.choice(np.arange(n), size=25)\n",
    "    for i in sample:\n",
    "        axi.plot(bin_vals, fun(intercepts[i], slopes[i], bin_vals), color=\"gray\", alpha=0.2, zorder=5)\n",
    "    axi.plot(bin_vals, fun(intercepts.mean(), slopes.mean(), bin_vals), color=\"black\", zorder=6)\n",
    "    print(bin_vals)\n",
    "\n",
    "    labels = pd.cut(degrade_df.disturb_intensity * 4, bins=bins).cat.categories\n",
    "    axi.set_xticks(bin_vals[[0,2,4,5,6,7,8]], labels=labels.astype(str)[[0,2,4,5,6,7,8]], rotation=-45)\n",
    "    axi.set_xlabel(f\"Disturbance intensity index\")\n",
    "    axi.set_ylabel(\"Median percent AGBD loss\")\n",
    "    axi.set_title(f\"Biomass loss by disturbance intensity\")\n",
    "\n",
    "    legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='tab:orange', label=\"Treatment\",\n",
    "                markerfacecolor='tab:orange', markersize=7, linestyle='None'),\n",
    "            Line2D([0], [0], color='black', linewidth=3, label=f\"y = {slopes.mean():.1f} * sqrt(x) + {intercepts.mean():.1f}\\n$R^2$ = {rs.mean():.2f}, p = {pvals.mean():.4f}\"),\n",
    "        ]\n",
    "    axi.legend(bbox_to_anchor=(1.0, 0.215), handles=legend_elements)\n",
    "    print(rs.mean())\n",
    "    print(pvals.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "################################################################################\n",
    "# A. GLAD disturbance extent\n",
    "################################################################################\n",
    "axi = ax[0][0]\n",
    "bootstrap_and_plot_extent(glad_df, axi)\n",
    "\n",
    "################################################################################\n",
    "# B. AFC disturbance extent\n",
    "################################################################################\n",
    "axi = ax[0][1]\n",
    "bootstrap_and_plot_extent(afc_df, axi)\n",
    "\n",
    "################################################################################\n",
    "# C. GLAD time since disturbance\n",
    "################################################################################\n",
    "axi = ax[1][0]\n",
    "bootstrap_and_plot_time(days_since_disturb_df, axi)\n",
    "\n",
    "################################################################################\n",
    "# D. AFC disturbance intensity\n",
    "################################################################################\n",
    "axi = ax[1][1]\n",
    "bootstrap_and_plot_intensity(intensity_df, bins, axi)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there is still statistically significant positive correlation\n",
    "# even if the disturbance extent is constant.\n",
    "from scipy.stats import pearsonr\n",
    "bins = np.concatenate([[0.0, 0.05], np.arange(0.1,0.61, 0.1)])\n",
    "print(bins)\n",
    "intensity_df[\"disturb_bin\"] = pd.cut(intensity_df.disturb_intensity, bins=bins, labels=bins[:-1])\n",
    "k_dist = len(bins) - 1\n",
    "k_extent = np.max(intensity_df.measured_disturbance)\n",
    "prs = []\n",
    "for extent in range(1,k_extent+1):\n",
    "    tdf = intensity_df[intensity_df.measured_disturbance == extent]\n",
    "    print(tdf.disturb_bin.value_counts())\n",
    "    n = 100\n",
    "    medians = np.zeros((n, k_dist))\n",
    "    for i in range(n):\n",
    "        for j, b in enumerate(bins[:-1]):\n",
    "            medians[i, j] = np.median(tdf[tdf.disturb_bin == b].agbd_pct_loss.sample(frac=1, replace=True))\n",
    "    print(medians.mean(axis=0))\n",
    "    prs.append(pearsonr(bins[:-1], medians.mean(axis=0)))\n",
    "\n",
    "for pr in prs:\n",
    "    print(pr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
